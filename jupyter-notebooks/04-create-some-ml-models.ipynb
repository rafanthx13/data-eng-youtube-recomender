{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ML Model\n",
    "\n",
    "## Entrada\n",
    "\n",
    "Arquivo com todas as label preenchdidas eplas etapas anteriores\n",
    "\n",
    "## Quando se faz Hyper-turning\n",
    "\n",
    "É somente depois de tentar várias possibildiades com os mdelos. O hyper-turning nâo faz milagre, entao só deixa pro final.\n",
    "\n",
    "## Descriçâo das aulas\n",
    "\n",
    "31 - Inicio do Notebook e testando hyer-turning no RandomForest\n",
    "\n",
    "32 - LightGBM e Sua hyperturning\n",
    "+ As formas de fazer hipertuning são:\n",
    "   - GridSearhc: ALém de demorada nâo tem bons resultados pois é manual e nao sabemos até que potnao estamos atingindo um bom valor o hiper-parametro que causae uma mudança positiva\n",
    "   - RandomSearch: Tende a ser melhor que GridSearch\n",
    "   - Baysean Optimization (AutoML): Uma nova abordagem que vamos usar nesse notebook.\n",
    "     * É uma busca alertória mas guiada de forma inteligente\n",
    "     \n",
    "33 - Logistic Regression\n",
    "+ Vamos aprender como analisar as nossa features para a regresão logistica\n",
    "  - Aprensetou bons resultados\n",
    "  \n",
    "### Tabela de Score\n",
    "````\n",
    "RandomForest\n",
    " avg_precision :: 0.3876787322730141 ; roc_auc :: 0.6775564963329557\n",
    "LightGBM - No Hyper-Tuning\n",
    " avg_precision :: 0.358824358569258  ; roc_auc :: 0.644678313755888\n",
    "LightBGM com Baysian Optimizer\n",
    "'avg_precision': 0.40242103560963294 ; 'roc': 0.6723749329199212, \n",
    "Logistic REgression = C=0.5 StandardScaler\n",
    "(0.41578009994342235, 0.6897143879315486) -  \n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "pd.set_option(\"max.columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar e analisar DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1410, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"raw_data_all_labeled2.csv\", index_col=0).dropna(subset=['y'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sem duplicata\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sem duplicata nos titulos\n",
    "df.duplicated(['title']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1410 entries, 0 to 1471\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   uploader       1410 non-null   object \n",
      " 1   title          1410 non-null   object \n",
      " 2   y              1410 non-null   float64\n",
      " 3   upload_date    1410 non-null   object \n",
      " 4   user           1410 non-null   object \n",
      " 5   view_count     1410 non-null   int64  \n",
      " 6   like_count     0 non-null      float64\n",
      " 7   dislike_count  0 non-null      float64\n",
      " 8   thumbnail      1410 non-null   object \n",
      " 9   width          1410 non-null   int64  \n",
      " 10  height         1410 non-null   int64  \n",
      " 11  categories     1410 non-null   object \n",
      " 12  tags           1167 non-null   object \n",
      " 13  channel_url    1410 non-null   object \n",
      " 14  description    1410 non-null   object \n",
      "dtypes: float64(3), int64(3), object(9)\n",
      "memory usage: 176.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uploader</th>\n",
       "      <th>title</th>\n",
       "      <th>y</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>user</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>categories</th>\n",
       "      <th>tags</th>\n",
       "      <th>channel_url</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yanjun Qi</td>\n",
       "      <td>S0-Introduction-Module3: Deep Learning and AI ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>UCHMYETgeGbNHVHLidZSV8BQ</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.ytimg.com/vi/LkPmTGw1jqo/hqdefault.j...</td>\n",
       "      <td>1280</td>\n",
       "      <td>672</td>\n",
       "      <td>Science &amp; Technology</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>http://www.youtube.com/channel/UCHMYETgeGbNHVH...</td>\n",
       "      <td>Course Web: \\nhttps://qiyanjun.github.io/2020f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ciência dos Dados</td>\n",
       "      <td>Machine Learning no Ensino Médio</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>UCd3ThZLzVDDnKSZMsbK0icg</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.ytimg.com/vi_webp/R_gBq8IfwJc/maxres...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1080</td>\n",
       "      <td>Education</td>\n",
       "      <td>machine learning|data science</td>\n",
       "      <td>http://www.youtube.com/channel/UCd3ThZLzVDDnKS...</td>\n",
       "      <td>A matemática, sempre ela....\\n\\nDe uma maneira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iKennyHD</td>\n",
       "      <td>NBA LIVE 22: EA COULD USE DEEP MACHINE LEARNIN...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>KennyCallOfDuty</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.ytimg.com/vi/Tix2xon9MSs/maxresdefau...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1080</td>\n",
       "      <td>Gaming</td>\n",
       "      <td>iKennyHD|nba live20|nba live 20|nba 2k20|live2...</td>\n",
       "      <td>http://www.youtube.com/channel/UCGMtoj9V9Go_im...</td>\n",
       "      <td>Wanna Donate? paypal.me/iKennyYT is where you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon Web Services</td>\n",
       "      <td>Amazon Aurora Machine Learning – SageMaker Int...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>AmazonWebServices</td>\n",
       "      <td>335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.ytimg.com/vi/w-2ip78NxAw/maxresdefau...</td>\n",
       "      <td>1920</td>\n",
       "      <td>1080</td>\n",
       "      <td>Science &amp; Technology</td>\n",
       "      <td>AWS|Amazon Web Services|Cloud|AWS Cloud|Cloud ...</td>\n",
       "      <td>http://www.youtube.com/channel/UCd6MoB9NC6uYN2...</td>\n",
       "      <td>Learn how you can turn relational data into in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GMRIT, Rajam, AP</td>\n",
       "      <td>Machine Learning and Deep Learning Implementat...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>UC8g7hz4oXFzXNryt8h1gRPw</td>\n",
       "      <td>1486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.ytimg.com/vi/f6XIY_M7FlA/hqdefault.j...</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.youtube.com/channel/UC8g7hz4oXFzXNr...</td>\n",
       "      <td>Resource Person\\nMr.S.Aravinth Seshadri\\nCerti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              uploader                                              title  \\\n",
       "0            Yanjun Qi  S0-Introduction-Module3: Deep Learning and AI ...   \n",
       "1    Ciência dos Dados                   Machine Learning no Ensino Médio   \n",
       "2             iKennyHD  NBA LIVE 22: EA COULD USE DEEP MACHINE LEARNIN...   \n",
       "3  Amazon Web Services  Amazon Aurora Machine Learning – SageMaker Int...   \n",
       "4     GMRIT, Rajam, AP  Machine Learning and Deep Learning Implementat...   \n",
       "\n",
       "     y upload_date                      user  view_count  like_count  \\\n",
       "0  0.0  2020-08-25  UCHMYETgeGbNHVHLidZSV8BQ          22         NaN   \n",
       "1  0.0  2020-08-25  UCd3ThZLzVDDnKSZMsbK0icg           3         NaN   \n",
       "2  0.0  2020-08-25           KennyCallOfDuty          47         NaN   \n",
       "3  0.0  2020-08-25         AmazonWebServices         335         NaN   \n",
       "4  1.0  2020-08-25  UC8g7hz4oXFzXNryt8h1gRPw        1486         NaN   \n",
       "\n",
       "   dislike_count                                          thumbnail  width  \\\n",
       "0            NaN  https://i.ytimg.com/vi/LkPmTGw1jqo/hqdefault.j...   1280   \n",
       "1            NaN  https://i.ytimg.com/vi_webp/R_gBq8IfwJc/maxres...   1920   \n",
       "2            NaN  https://i.ytimg.com/vi/Tix2xon9MSs/maxresdefau...   1920   \n",
       "3            NaN  https://i.ytimg.com/vi/w-2ip78NxAw/maxresdefau...   1920   \n",
       "4            NaN  https://i.ytimg.com/vi/f6XIY_M7FlA/hqdefault.j...   1280   \n",
       "\n",
       "   height            categories  \\\n",
       "0     672  Science & Technology   \n",
       "1    1080             Education   \n",
       "2    1080                Gaming   \n",
       "3    1080  Science & Technology   \n",
       "4     720        People & Blogs   \n",
       "\n",
       "                                                tags  \\\n",
       "0                                   Machine Learning   \n",
       "1                      machine learning|data science   \n",
       "2  iKennyHD|nba live20|nba live 20|nba 2k20|live2...   \n",
       "3  AWS|Amazon Web Services|Cloud|AWS Cloud|Cloud ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         channel_url  \\\n",
       "0  http://www.youtube.com/channel/UCHMYETgeGbNHVH...   \n",
       "1  http://www.youtube.com/channel/UCd3ThZLzVDDnKS...   \n",
       "2  http://www.youtube.com/channel/UCGMtoj9V9Go_im...   \n",
       "3  http://www.youtube.com/channel/UCd6MoB9NC6uYN2...   \n",
       "4  http://www.youtube.com/channel/UC8g7hz4oXFzXNr...   \n",
       "\n",
       "                                         description  \n",
       "0  Course Web: \\nhttps://qiyanjun.github.io/2020f...  \n",
       "1  A matemática, sempre ela....\\n\\nDe uma maneira...  \n",
       "2  Wanna Donate? paypal.me/iKennyYT is where you ...  \n",
       "3  Learn how you can turn relational data into in...  \n",
       "4  Resource Person\\nMr.S.Aravinth Seshadri\\nCerti...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Limpeza das datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo = pd.DataFrame(index=df.index)\n",
    "df_limpo['title'] = df['title']\n",
    "df_limpo['date'] = pd.to_datetime(df['upload_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Limpeza de views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = df['view_count'].fillna(0)\n",
    "df_limpo['views'] = views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As features vao ficar num dataset separado\n",
    "features = pd.DataFrame(index=df_limpo.index)\n",
    "\n",
    "# Y (target) tambem fica separado\n",
    "y = df['y'].copy()\n",
    "\n",
    "# Feature Enginneering\n",
    "today = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "features['tempo_desde_pub'] = (pd.to_datetime(today) -  df_limpo['date']) / np.timedelta64(1, 'D')\n",
    "features['views'] = df_limpo['views']\n",
    "features['views_por_dia'] = (features['views'] / features['tempo_desde_pub']).round(3)\n",
    "features.drop(['tempo_desde_pub'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test\n",
    "\n",
    "Vamos dividir pela data, de forma que divida ao meioa s duas quantidade de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((714, 2), (696, 2), (714,), (696,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_train = df_limpo['date'] < '2020-03-10'\n",
    "mask_val = df_limpo['date'] >= '2020-03-10'\n",
    "\n",
    "Xtrain, Xval = features[mask_train], features[mask_val]\n",
    "ytrain, yval = y[mask_train], y[mask_val]\n",
    "Xtrain.shape, Xval.shape, ytrain.shape, yval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views</th>\n",
       "      <th>views_por_dia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>41425</td>\n",
       "      <td>64.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     views  views_por_dia\n",
       "253  41425         64.125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253    0.0\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando com apenas uma feature (NLTK)\n",
    "\n",
    "Vamos usar um Vecotorizer para tokenizar os títulos e assim tentarmos fazer o treinamento só com palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscando somente o 'title' para fazer a tokenização\n",
    "title_train = df_limpo[mask_train]['title']\n",
    "title_val = df_limpo[mask_val]['title']\n",
    "\n",
    "# Min df - minimo de vezes que palavra tem que aparecer pra virar coluna\n",
    "#        => min_df=2 quer dizer enta que a palavra tem que aparecer 2 vezes\n",
    "#           no minimo para ela representar uma nova coluna\n",
    "## ngram_range = vai fazer colunas para tanto 1 word \"machine\" quanto 2 juntas \"machine learning\"\n",
    "\n",
    "title_vec = TfidfVectorizer(min_df=2, ngram_range=(1,2))\n",
    "\n",
    "## Aplicando tokenizador\n",
    "title_bow_train = title_vec.fit_transform(title_train)\n",
    "title_bow_val = title_vec.transform(title_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convertemos o nosso titulo para um vetor de tamanho 1134, sendo esas a quantidade de palavras distintas**\n",
    "\n",
    "Como é um array esparso (ou seja, esta preenhcido de muito zero) é difícil por isso na tela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714, 1134)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_bow_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1134 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_bow_train[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenando as variávels numéricas com as geradas pelo TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((714, 1136), (696, 1136))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_wtitle = sparse.hstack([Xtrain, title_bow_train])\n",
    "Xval_wtitle = sparse.hstack([Xval, title_bow_val])\n",
    "Xtrain_wtitle.shape, Xval_wtitle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', n_estimators=1000, n_jobs=4,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = RandomForestClassifier(n_estimators=1000,\n",
    "                             min_samples_leaf=1, \n",
    "                             random_state=0, \n",
    "                             class_weight='balanced', \n",
    "                             n_jobs=4)\n",
    "mdl.fit(Xtrain_wtitle, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mdl.predict_proba(Xval_wtitle)[: ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      " avg_precision :: 0.3876787322730141 \n",
      " roc_auc :: 0.6775564963329557\n"
     ]
    }
   ],
   "source": [
    "print('RandomForest')\n",
    "print( \" avg_precision :: \" + str(metrics.average_precision_score(yval, p)), '\\n' ,\n",
    "      \"roc_auc :: \" + str(metrics.roc_auc_score(yval, p)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O melhor resultado que Mario encontrou para oe xemplo dele\n",
    "min_df=2 ngram_range=(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(class_weight='balanced', n_jobs=4, random_state=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = LGBMClassifier(random_state=0, class_weight='balanced', n_jobs=4)\n",
    "mdl.fit(Xtrain_wtitle, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rhavel/miniconda3/lib/python3.9/site-packages/lightgbm/basic.py:859: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    }
   ],
   "source": [
    "p = mdl.predict_proba(Xval_wtitle)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - No Hyper-turning\n",
      " avg_precision :: 0.358824358569258 \n",
      " roc_auc :: 0.644678313755888\n"
     ]
    }
   ],
   "source": [
    "# metrics.average_precision_score(yval, p), metrics.roc_auc_score(yval, p)\n",
    "print('LightGBM - No Hyper-turning')\n",
    "print( \" avg_precision :: \" + str(metrics.average_precision_score(yval, p)), '\\n' ,\n",
    "      \"roc_auc :: \" + str(metrics.roc_auc_score(yval, p)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bayesian Optimization\n",
    "\n",
    "É uma opçâo avançada ao fazer GridSearch que é demorado) e RandomSearch (é melhor que o grid mas nâo tanto quanto Baysian).\n",
    "\n",
    "**Bayesian Optimization É UM RANDOM-SEARCH OTIMIZADO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib: scikit optimizer\n",
    "from skopt import forest_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "def tune_lgbm(params):\n",
    "    # Set Hyper-params\n",
    "    tunning = {}\n",
    "    print()\n",
    "    print(params)\n",
    "    lr = params[0]\n",
    "    max_depth = params[1]\n",
    "    min_child_samples = params[2]\n",
    "    subsample = params[3]\n",
    "    colsample_bytree = params[4]\n",
    "    n_estimators = params[5]\n",
    "    min_df = params[6]\n",
    "    ngram_range = (1, params[7])\n",
    "    # Sety TDF-Vec hyper-params, generate vocabulary and apply over data\n",
    "    title_vec = TfidfVectorizer(min_df=min_df, ngram_range=ngram_range)\n",
    "    title_bow_train = title_vec.fit_transform(title_train)\n",
    "    title_bow_val = title_vec.transform(title_val)\n",
    "    # Join Numeric Features with TDF-Vec\n",
    "    Xtrain_wtitle = sparse.hstack([Xtrain, title_bow_train])\n",
    "    Xval_wtitle = sparse.hstack([Xval, title_bow_val])\n",
    "    # Create Model with hyper-params\n",
    "    mdl = LGBMClassifier(learning_rate=lr, num_leaves=2 ** max_depth, max_depth=max_depth,\n",
    "                         min_child_samples=min_child_samples, subsample=subsample,\n",
    "                         colsample_bytree=colsample_bytree, bagging_freq=1, n_estimators=n_estimators,\n",
    "                        random_state=0, class_weight='balanced', n_jobs=4)\n",
    "    # Fit Model\n",
    "    mdl.fit(Xtrain_wtitle, ytrain)\n",
    "    # Predict\n",
    "    pred = mdl.predict_proba(Xval_wtitle)[:, 1]\n",
    "    print(metrics.roc_auc_score(yval, pred))\n",
    "    # Save Results\n",
    "    tunning['params'] = params\n",
    "    tunning['roc'] = metrics.roc_auc_score(yval, pred)\n",
    "    tunning['avg_prec'] = metrics.average_precision_score(yval, pred)\n",
    "    results.append(tunning)\n",
    "    \n",
    "    # Esta negativa pois eu quero maximizar a average-precision\n",
    "    # como nao tem um 'skopt.forest_maximze' entao eu inverto a avg_precision para ter esse efeito de maximizar\n",
    "    return -metrics.average_precision_score(yval, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERVALOS DOS HYPER-PARAMETROS  DO LIGHT-GBM E TDF-VEC\n",
    "space = [(1e-3, 1e-1, 'log-uniform'), # learning-rate : usamos log-unifrom para ter mais chance de pegar valores pequenos\n",
    "         (1,10), # max_depth\n",
    "         (1,20), # min_child_samples\n",
    "         (0.05, 1.), # subsample\n",
    "         (0.05, 1.), # colsample_bytree\n",
    "         (100, 1000), # n_estimators\n",
    "         (1,5), # TDF-Vectorizer: min_df\n",
    "         (1,5)] # TDF-Vectorizer: ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EXEC skopt.forest_minimize (Baysian Optimization)\n",
    "res = forest_minimize(tune_lgbm, # funçâo apra testar score que voce quer MINIMIZAR\n",
    "                      space,  # hyper-paremtros e seu range\n",
    "                      random_state=160745,\n",
    "                      n_random_starts=20, # testar 20 vezes\n",
    "                      n_calls=50, \n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ END OF PROCESS ==============\n",
    "# Iteration No: 50 ended. Search finished for the next optimal point.\n",
    "# Time taken: 22.8565\n",
    "# Function value obtained: -0.3677\n",
    "# Current minimum: -0.4024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.003924937303997735,\n",
       " 10,\n",
       " 9,\n",
       " 0.42896739513988846,\n",
       " 0.08080324515701484,\n",
       " 196,\n",
       " 1,\n",
       " 5]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Melhores parametros encontrados pelo Baysian Optimizer\n",
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': [0.003924937303997735,\n",
       "  10,\n",
       "  9,\n",
       "  0.42896739513988846,\n",
       "  0.08080324515701484,\n",
       "  196,\n",
       "  1,\n",
       "  5],\n",
       " 'roc': 0.6723749329199212,\n",
       " 'avg_prec': 0.40242103560963294}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALém de mostra os parametros, mostra a precision e roc auc dos melhores paremetros\n",
    "sorted(results, key = lambda i: i['avg_prec'],reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Reg\n",
    "\n",
    "Vamos testar: Usando spo StandarScale e só MaxABsScaler. Par afazer isso, temos que comentar e descomentar os trehcos de código a seguir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rhavel/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:585: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/rhavel/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:585: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/rhavel/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:585: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/rhavel/miniconda3/lib/python3.9/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "Xtrain_wtitle2 = sparse.csr_matrix(Xtrain_wtitle.copy())\n",
    "Xval_wtitle2 = sparse.csr_matrix(Xval_wtitle.copy())\n",
    "\n",
    "scaler = StandardScaler() ## Para variaveis numericas\n",
    "# scaler = MaxAbsScaler() ## Para variaveis sparsas (o array gigante de vocabulario)\n",
    "\n",
    "Xtrain_wtitle2[: , :2] = scaler.fit_transform(Xtrain_wtitle2[:, :2].todense())\n",
    "Xval_wtitle2[:, :2] = scaler.transform(Xval_wtitle2[:, :2].todense())\n",
    "\n",
    "# Xtrain_wtitle2 = scaler.fit_transform(Xtrain_wtitle2)\n",
    "# Xval_wtitle2 = scaler.transform(Xval_wtitle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 1136)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xval_wtitle2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, n_jobs=4, random_state=4)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = LogisticRegression(C=0.5, n_jobs=4, random_state=4)\n",
    "mdl.fit(Xtrain_wtitle2, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mdl.predict_proba(Xval_wtitle2)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41578009994342235, 0.6897143879315486)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.average_precision_score(yval, p), metrics.roc_auc_score(yval, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rafael\n",
    "## (0.4147342546287094, 0.6589231411364855) - C=0.5 MaxAbScaler\n",
    "## (0.41578009994342235, 0.6897143879315486) -  C=0.5 StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mario Filho\n",
    "# (0.4043414314912761, 0.6789338739490788) - sem tunning StandardScaler\n",
    "# (0.3988238048468208, 0.6462226462345716) - sem tunning MaxAbScaler\n",
    "# (0.33826219541849384, 0.6082881163913899) - C=10, MaxAbScaler\n",
    "# (0.41472090277819385, 0.6588873650945083) - C=0.5 MaxAbScaler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
